{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ---------- small helpers ----------\n",
    "\n",
    "# Targets to train on (adjust this list if needed)\n",
    "TARGET_COLS = [\n",
    "    \"oc_usda.c729_w.pct\",\n",
    "    \"c.tot_usda.a622_w.pct\",\n",
    "    \"n.tot_usda.a623_w.pct\",\n",
    "    \"ph.h2o_usda.a268_index\",\n",
    "    \"ph.cacl2_usda.a481_index\",\n",
    "    \"cec_usda.a723_cmolc.kg\",\n",
    "    \"ec_usda.a364_ds.m\",\n",
    "    \"clay.tot_usda.a334_w.pct\",\n",
    "    \"sand.tot_usda.c60_w.pct\",\n",
    "    \"silt.tot_usda.c62_w.pct\",\n",
    "    \"bd_usda.a4_g.cm3\",\n",
    "    \"wr.10kPa_usda.a414_w.pct\",\n",
    "    \"wr.33kPa_usda.a415_w.pct\",\n",
    "    \"wr.1500kPa_usda.a417_w.pct\",\n",
    "    \"awc.33.1500kPa_usda.c80_w.frac\",\n",
    "    \"fe.ox_usda.a60_w.pct\",\n",
    "    \"al.ox_usda.a59_w.pct\",\n",
    "    \"fe.dith_usda.a66_w.pct\",\n",
    "    \"al.dith_usda.a65_w.pct\",\n",
    "    \"p.ext_usda.a1070_mg.kg\",\n",
    "    \"k.ext_usda.a1065_mg.kg\",\n",
    "    \"mg.ext_usda.a1066_mg.kg\",\n",
    "    \"ca.ext_usda.a1059_mg.kg\",\n",
    "    \"na.ext_usda.a1068_mg.kg\",\n",
    "]\n",
    "\n",
    "# Targets to train in log1p-space (transform back with expm1 for metrics/use)\n",
    "LOG_TRANSFORM_TARGETS = {\n",
    "    \"p.ext_usda.a1070_mg.kg\",\n",
    "    \"k.ext_usda.a1065_mg.kg\",\n",
    "    \"mg.ext_usda.a1066_mg.kg\",\n",
    "    \"ca.ext_usda.a1059_mg.kg\",\n",
    "    \"na.ext_usda.a1068_mg.kg\",\n",
    "    \"fe.dith_usda.a66_w.pct\",\n",
    "    \"al.dith_usda.a65_w.pct\",\n",
    "    \"fe.ox_usda.a60_w.pct\",\n",
    "    \"al.ox_usda.a59_w.pct\",\n",
    "    \"s.ext_mel3_mg.kg\",\n",
    "    \"s.tot_usda.a624_w.pct\",\n",
    "    \"caco3_usda.a54_w.pct\",\n",
    "}\n",
    "\n",
    "\n",
    "def snv_transform(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply Standard Normal Variate (SNV) row-wise to spectra.\n",
    "    X: (n_samples, n_features) numeric array.\n",
    "    \"\"\"\n",
    "    mean = X.mean(axis=1, keepdims=True)\n",
    "    std = X.std(axis=1, keepdims=True)\n",
    "    # Avoid division by zero\n",
    "    std[std == 0] = 1.0\n",
    "    return (X - mean) / std\n",
    "\n",
    "\n",
    "def build_feature_matrix_per_sample(\n",
    "    df: pd.DataFrame,\n",
    "    extra_numeric_cols: List[str] = None,\n",
    "    n_pca_components: int = 120,\n",
    ") -> Tuple[np.ndarray, PCA, SimpleImputer, SimpleImputer, StandardScaler, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Build the feature matrix X from the cleaned OSSL dataframe.\n",
    "\n",
    "    Steps:\n",
    "    - Select spectral columns: VisNIR (scan_visnir.*_ref) + MIR (scan_mir.*_abs)\n",
    "    - Impute missing spectral values with column medians\n",
    "    - Apply SNV (Standard Normal Variate) per row\n",
    "    - PCA-compress to n_pca_components (or fewer if not enough bands)\n",
    "    - Optionally add extra numeric columns (imputed + scaled)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features_total)\n",
    "    pca : PCA\n",
    "        Fitted PCA object on SNV spectra.\n",
    "    spec_imputer : SimpleImputer\n",
    "        Imputer used on spectral columns.\n",
    "    num_imputer : SimpleImputer\n",
    "        Imputer used on extra numeric columns.\n",
    "    num_scaler : StandardScaler\n",
    "        Scaler used on extra numeric columns.\n",
    "    spectral_cols : list of str\n",
    "        Names of spectral columns used.\n",
    "    extra_used : list of str\n",
    "        Extra numeric columns actually included.\n",
    "    \"\"\"\n",
    "    # 1. Spectral columns\n",
    "    spectral_cols = [\n",
    "        c for c in df.columns\n",
    "        if (c.startswith(\"scan_visnir.\") and c.endswith(\"_ref\"))\n",
    "        or (c.startswith(\"scan_mir.\") and c.endswith(\"_abs\"))\n",
    "    ]\n",
    "    if len(spectral_cols) == 0:\n",
    "        raise ValueError(\"No spectral columns found (scan_visnir.*_ref / scan_mir.*_abs).\")\n",
    "\n",
    "    spec_imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_spec = spec_imputer.fit_transform(df[spectral_cols].astype(float))\n",
    "\n",
    "    # 2. SNV (row-wise)\n",
    "    spec_mean = X_spec.mean(axis=1, keepdims=True)\n",
    "    spec_std = X_spec.std(axis=1, keepdims=True)\n",
    "    spec_std[spec_std == 0] = 1.0\n",
    "    X_spec_snv = (X_spec - spec_mean) / spec_std\n",
    "\n",
    "    # 3. PCA\n",
    "    n_components = min(n_pca_components, X_spec_snv.shape[1])\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_spec_pcs = pca.fit_transform(X_spec_snv)\n",
    "\n",
    "    # 4. Extra numeric columns (depth, lat/long)\n",
    "    if extra_numeric_cols is None:\n",
    "        extra_numeric_cols = [\n",
    "            \"layer.upper.depth_usda_cm\",\n",
    "            \"layer.lower.depth_usda_cm\",\n",
    "            \"latitude.point_wgs84_dd\",\n",
    "            \"longitude.point_wgs84_dd\",\n",
    "        ]\n",
    "\n",
    "    extra_used = [c for c in extra_numeric_cols if c in df.columns]\n",
    "    if len(extra_used) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        X_num = num_imputer.fit_transform(df[extra_used].astype(float))\n",
    "        num_scaler = StandardScaler()\n",
    "        X_num_scaled = num_scaler.fit_transform(X_num)\n",
    "        X = np.hstack([X_spec_pcs, X_num_scaled])\n",
    "    else:\n",
    "        num_imputer = None\n",
    "        num_scaler = None\n",
    "        X = X_spec_pcs\n",
    "\n",
    "    return X, pca, spec_imputer, num_imputer, num_scaler, spectral_cols, extra_used\n",
    "\n",
    "\n",
    "\n",
    "def select_targets(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Choose a sensible default list of target soil properties based on\n",
    "    what we discussed (SOC, texture, pH, CEC, WR, etc.) that exist in df.\n",
    "    \"\"\"\n",
    "    candidate_targets = [\n",
    "        # Chemistry\n",
    "        \"oc_usda.c729_w.pct\",\n",
    "        \"c.tot_usda.a622_w.pct\",\n",
    "        \"n.tot_usda.a623_w.pct\",\n",
    "        \"ph.h2o_usda.a268_index\",\n",
    "        \"ph.cacl2_usda.a481_index\",\n",
    "        \"cec_usda.a723_cmolc.kg\",\n",
    "        \"ec_usda.a364_ds.m\",\n",
    "        # Texture & physical\n",
    "        \"clay.tot_usda.a334_w.pct\",\n",
    "        \"sand.tot_usda.c60_w.pct\",\n",
    "        \"silt.tot_usda.c62_w.pct\",\n",
    "        \"bd_usda.a4_g.cm3\",\n",
    "        # Water retention\n",
    "        \"wr.10kPa_usda.a414_w.pct\",\n",
    "        \"wr.33kPa_usda.a415_w.pct\",\n",
    "        \"wr.1500kPa_usda.a417_w.pct\",\n",
    "        \"awc.33.1500kPa_usda.c80_w.frac\",\n",
    "        # Fe/Al oxides\n",
    "        \"fe.ox_usda.a60_w.pct\",\n",
    "        \"al.ox_usda.a59_w.pct\",\n",
    "        \"fe.dith_usda.a66_w.pct\",\n",
    "        \"al.dith_usda.a65_w.pct\",\n",
    "        # Nutrients (weaker but useful)\n",
    "        \"p.ext_usda.a1070_mg.kg\",\n",
    "        \"k.ext_usda.a1065_mg.kg\",\n",
    "        \"mg.ext_usda.a1066_mg.kg\",\n",
    "        \"ca.ext_usda.a1059_mg.kg\",\n",
    "        \"na.ext_usda.a1068_mg.kg\",\n",
    "    ]\n",
    "\n",
    "    target_cols = [c for c in candidate_targets if c in df.columns]\n",
    "    if not target_cols:\n",
    "        raise ValueError(\"No candidate target columns found in DataFrame.\")\n",
    "    return target_cols\n",
    "\n",
    "\n",
    "def learning_rate_decay(initial_lr=0.05, decay=0.95):\n",
    "    \"\"\"\n",
    "    Returns a function suitable for XGBoost's LearningRateScheduler callback.\n",
    "    lr(t) = initial_lr * decay^t\n",
    "    \"\"\"\n",
    "    def _lr(step: int) -> float:\n",
    "        return initial_lr * (decay ** step)\n",
    "    return _lr\n",
    "\n",
    "\n",
    "def evaluate_multioutput(\n",
    "    y_true_trans: np.ndarray,\n",
    "    y_pred_trans: np.ndarray,\n",
    "    target_names: List[str],\n",
    "    log_transform_targets: set,\n",
    "):\n",
    "    \"\"\"\n",
    "    Print per-target R2 and RMSE on the ORIGINAL scale.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true_trans : array, shape (n_samples, n_targets)\n",
    "        Ground truth in the same (possibly log-transformed) space used for training.\n",
    "    y_pred_trans : array, shape (n_samples, n_targets)\n",
    "        Predictions in the same (possibly log-transformed) space.\n",
    "    target_names : list of str\n",
    "        Column names corresponding to each target index.\n",
    "    log_transform_targets : set of str\n",
    "        Names of targets that were trained in log1p space and must be\n",
    "        back-transformed with expm1 for metric computation.\n",
    "    \"\"\"\n",
    "    r2_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    for i, name in enumerate(target_names):\n",
    "        yt = y_true_trans[:, i]\n",
    "        yp = y_pred_trans[:, i]\n",
    "\n",
    "        # Back to original scale if this target was log-transformed\n",
    "        if name in log_transform_targets:\n",
    "            yt = np.expm1(yt)\n",
    "            yp = np.expm1(yp)\n",
    "\n",
    "        r2 = r2_score(yt, yp)\n",
    "        rmse = mean_squared_error(yt, yp, squared=False)\n",
    "        r2_list.append(r2)\n",
    "        rmse_list.append(rmse)\n",
    "        print(f\"  {name:30s}  R2 = {r2:6.3f}   RMSE = {rmse:8.3f}\")\n",
    "\n",
    "    print(\"\\n  ---- Averages ----\")\n",
    "    print(f\"  Mean R2   : {np.nanmean(r2_list):6.3f}\")\n",
    "    print(f\"  Mean RMSE : {np.nanmean(rmse_list):8.3f}\")\n",
    "\n",
    "\n",
    "# ---------- main pipeline ----------\n",
    "\n",
    "def train_per_target_models(\n",
    "    csv_path: str,\n",
    "    target_cols: List[str] = TARGET_COLS,\n",
    "    log_transform_targets: set = LOG_TRANSFORM_TARGETS,\n",
    "    test_size: float = 0.2,\n",
    "    n_splits_cv: int = 5,\n",
    "    n_pca_components: int = 120,\n",
    "    min_rows_required: int = 200,\n",
    "    use_gpu: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    OSSL-style training: one XGBoost model per soil property, each using all rows\n",
    "    where that property is non-missing. Uses shared SNV+PCA spectral features.\n",
    "\n",
    "    Returns a dict with:\n",
    "        - models:  dict[target] -> fitted XGBRegressor\n",
    "        - metrics: dict[target] -> CV/test metrics\n",
    "        - pca, spec_imputer, num_imputer, num_scaler\n",
    "        - spectral_cols, extra_used\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    print(f\"Loaded cleaned dataset: {df.shape} (rows, columns)\")\n",
    "\n",
    "    # Build shared feature matrix\n",
    "    X, pca, spec_imputer, num_imputer, num_scaler, spec_cols, extra_used = build_feature_matrix_per_sample(\n",
    "        df,\n",
    "        n_pca_components=n_pca_components,\n",
    "    )\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"  Spectral columns used: {len(spec_cols)}\")\n",
    "    print(f\"  Extra numeric columns used: {extra_used}\")\n",
    "\n",
    "    models: Dict[str, XGBRegressor] = {}\n",
    "    metrics: Dict[str, dict] = {}\n",
    "\n",
    "    # ---- XGBoost factory ----\n",
    "    def make_xgb() -> XGBRegressor:\n",
    "        params = dict(\n",
    "            n_estimators=600,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        if use_gpu:\n",
    "            params.update(\n",
    "                tree_method=\"hist\",  # with device=\"cuda\" this uses GPU in XGB >= 2.0\n",
    "                device=\"cuda\",\n",
    "            )\n",
    "        else:\n",
    "            params.update(\n",
    "                tree_method=\"hist\",\n",
    "                device=\"cpu\",\n",
    "            )\n",
    "        return XGBRegressor(**params)\n",
    "\n",
    "    for target in target_cols:\n",
    "        if target not in df.columns:\n",
    "            print(f\"\\n[SKIP] Target {target} not found in dataframe columns.\")\n",
    "            continue\n",
    "\n",
    "        y = df[target].to_numpy(dtype=float)\n",
    "        mask = ~np.isnan(y)\n",
    "        n_available = int(mask.sum())\n",
    "\n",
    "        print(f\"\\n=== Target: {target} | available samples: {n_available} ===\")\n",
    "\n",
    "        if n_available < min_rows_required:\n",
    "            print(f\"  [SKIP] Only {n_available} non-missing rows (< {min_rows_required}).\")\n",
    "            continue\n",
    "\n",
    "        X_t = X[mask]\n",
    "        y_t = y[mask]\n",
    "\n",
    "        is_log = target in log_transform_targets\n",
    "        if is_log:\n",
    "            y_t = np.log1p(y_t)\n",
    "\n",
    "        X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "            X_t, y_t, test_size=test_size, random_state=42\n",
    "        )\n",
    "\n",
    "        # CV\n",
    "        kf = KFold(n_splits=n_splits_cv, shuffle=True, random_state=42)\n",
    "        cv_r2_list, cv_rmse_list = [], []\n",
    "\n",
    "        for fold, (tr_idx, val_idx) in enumerate(kf.split(X_trainval), start=1):\n",
    "            X_tr, X_val = X_trainval[tr_idx], X_trainval[val_idx]\n",
    "            y_tr, y_val = y_trainval[tr_idx], y_trainval[val_idx]\n",
    "\n",
    "            model = make_xgb()\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            if is_log:\n",
    "                y_val_true_orig = np.expm1(y_val)\n",
    "                y_val_pred_orig = np.expm1(y_val_pred)\n",
    "            else:\n",
    "                y_val_true_orig = y_val\n",
    "                y_val_pred_orig = y_val_pred\n",
    "\n",
    "            r2 = r2_score(y_val_true_orig, y_val_pred_orig)\n",
    "            rmse = mean_squared_error(y_val_true_orig, y_val_pred_orig, squared=False)\n",
    "            cv_r2_list.append(r2)\n",
    "            cv_rmse_list.append(rmse)\n",
    "\n",
    "        cv_r2_mean = float(np.mean(cv_r2_list))\n",
    "        cv_rmse_mean = float(np.mean(cv_rmse_list))\n",
    "        print(f\"  CV (mean over {n_splits_cv} folds): R2 = {cv_r2_mean:.3f}, RMSE = {cv_rmse_mean:.3f}\")\n",
    "\n",
    "        # Final model on train+val\n",
    "        final_model = make_xgb()\n",
    "        final_model.fit(X_trainval, y_trainval)\n",
    "\n",
    "        # Test\n",
    "        y_test_pred = final_model.predict(X_test)\n",
    "        if is_log:\n",
    "            y_test_true_orig = np.expm1(y_test)\n",
    "            y_test_pred_orig = np.expm1(y_test_pred)\n",
    "        else:\n",
    "            y_test_true_orig = y_test\n",
    "            y_test_pred_orig = y_test_pred\n",
    "\n",
    "        test_r2 = float(r2_score(y_test_true_orig, y_test_pred_orig))\n",
    "        test_rmse = float(mean_squared_error(y_test_true_orig, y_test_pred_orig, squared=False))\n",
    "        print(f\"  Test: R2 = {test_r2:.3f}, RMSE = {test_rmse:.3f}\")\n",
    "\n",
    "        models[target] = final_model\n",
    "        metrics[target] = {\n",
    "            \"n_samples\": n_available,\n",
    "            \"cv_r2_mean\": cv_r2_mean,\n",
    "            \"cv_rmse_mean\": cv_rmse_mean,\n",
    "            \"test_r2\": test_r2,\n",
    "            \"test_rmse\": test_rmse,\n",
    "            \"log_transformed\": is_log,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"models\": models,\n",
    "        \"metrics\": metrics,\n",
    "        \"pca\": pca,\n",
    "        \"spec_imputer\": spec_imputer,\n",
    "        \"num_imputer\": num_imputer,\n",
    "        \"num_scaler\": num_scaler,\n",
    "        \"spectral_cols\": spec_cols,\n",
    "        \"extra_used\": extra_used,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_per_target_models(\n",
    "    \"/kaggle/input/ossl-cleaned/ossl_cleaned.csv\",\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "models = results[\"models\"]\n",
    "metrics = results[\"metrics\"]\n",
    "pca = results[\"pca\"]\n",
    "spec_imputer = results[\"spec_imputer\"]\n",
    "num_imputer = results[\"num_imputer\"]\n",
    "num_scaler = results[\"num_scaler\"]\n",
    "spectral_cols = results[\"spectral_cols\"]\n",
    "extra_cols = results[\"extra_used\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1bd397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"models\": models,\n",
    "        \"metrics\": metrics,\n",
    "        \"pca\": pca,\n",
    "        \"spec_imputer\": spec_imputer,\n",
    "        \"num_imputer\": num_imputer,\n",
    "        \"num_scaler\": num_scaler,\n",
    "        \"spectral_cols\": spectral_cols,\n",
    "        \"extra_cols\": extra_cols,\n",
    "        \"log_transform_targets\": list(LOG_TRANSFORM_TARGETS),\n",
    "    },\n",
    "    \"ossl_per_target_pipeline.pkl\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
